{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='font-family: CMU Sans Serif, sans-serif;'> Hidden units  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This issue is unique to feedforward neural networks: how to choose the type of hidden unit to use in the hidden layers of the model.\n",
    "\n",
    "**ReLU are generally acceptable choices**. The following gives intuition on how to select what hidden units, however predicting what will work best before fitting the model is usually impossible.\n",
    "\n",
    "Some hidden units below are not differentiable at all points (e.g. ReLU $g(z) = \\max\\{ 0,z \\}$). This is not considered an issue since we do not expect to arrive at the global minima of the loss function, but merely reduce it significantly. Additionally, non-differentiable hidden units are usually only non-differentiable at few points and thus have left and right derivatives. Software returns either the left or right derivative in case of an undefined derivative (the justification is that we rarely have fx. $g(0)$ in the real world, but rather $g(0 + \\epsilon)$ which is rounded to $0$).\n",
    "\n",
    "Unless indicated otherwise, hidden units accept a vector of inputs $\\mathbf{x}$, computes an affine transformation $\\mathbf{z} = \\mathbf{W}^{\\top}\\mathbf{x} + \\mathbf{b}$, and then applies an element-wise nonlinear function $g(\\mathbf{z})$ where $g(\\cdot )$ distinguishes the hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family: CMU Sans Serif, sans-serif;'> Rectified linear units and their generalizations  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU is given by $g(z) = \\max\\{ 0,z \\}$. \n",
    "\n",
    "These are easy to optimize because they are similar to linear units. In half of its domain it is $0$ which makes the derivative large whenever the the unit is active. The second derivative is almost $0$ everywhere, and the derivative is $1$ everywhere that the unit is active, and zero otherwise. This means the gradient is far more useful for learning than it would be with activation function that introduce second-order effect.\n",
    "\n",
    "These are typically used on top of an affine transformation\n",
    "$$\n",
    "\\mathbf{h} = g(\\mathbf{W}^{\\top}\\mathbf{x} + \\mathbf{b}) \\tag{1}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

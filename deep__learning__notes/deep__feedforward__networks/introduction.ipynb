{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='font-family: CMU Sans Serif, sans-serif;'> Deep feedforward networks  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following notes come from this book MIT-book: https://www.deeplearningbook.org/\n",
    "\n",
    "This type of architecture includes no feedback connections where output from the model are fed back into itself. When this is the case we are working with recurrent neural networks.\n",
    "\n",
    "These models are represented by composing many different functions. The model is associated with a directed acyclic graph describing how the functions are composed. Consider a three layer network consisting of $f^{(1)}$, $f^{(2)}$, and $f^{(3)}$, then the network takes the form $f(\\mathbf{x}) = f^{(3)}(f^{(2)}(f^{(1)}(\\mathbf{x})))$. The number of layers make up the **depth** of the model. the final layer is the **output** layer. The number of neurons in each layer specify the **width** of the model. \n",
    "\n",
    "In training we drive $f(\\mathbf{x})$ close to $f^{*}(\\mathbf{x})$, and the training examples specify what th e the output layer must do at each point $\\mathbf{x}$; produce a values close to the label $y$. \n",
    "\n",
    "Feedforward neural nets can handle non-linear relationships by introducing the $\\phi$ mapping. $\\phi$ can be thought of providing a set of features describing $\\mathbf{x}$, or as providing a new representation of $\\mathbf{x}$. We want to learn this mapping $\\phi$. Consider the deep feedforward model with $\\phi$ as the hidden layer  $y = f(\\mathbf{x}; \\mathbf{\\theta}, \\mathbf{w}) = \\phi(\\mathbf{x};\\mathbf{\\theta})^{\\top}\\mathbf{w}$, where we have parameters $\\mathbf{\\theta}$ we can use to learn $\\phi$ from a broad class of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family: CMU Sans Serif, sans-serif;'> Example: Learning XOR  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XOR function takes to binary inputs $x_1$ and $x_2$ and returns $1$ if one of the inputs is $1$, and $0$ otherwise. This is the function we want to learn $y = f^{*}(\\mathbf{x})$. Our model is $y = f(\\mathbf{x};\\mathbf{\\theta})$, and the learning algorithm will adapts the parameters of $\\mathbf{\\theta}$ to make $f$ close to $f^{*}$. \n",
    "\n",
    "We want the network to perform correctly on four different points $\\mathbb{X} = \\{ [0,0]^{\\top}, [0,1]^{\\top}, [1,0]^{\\top}, [1,1]^{\\top} \\}$. The network will be trained on these points.\n",
    "\n",
    "For simplicity we choose the mean squared error to calculate the loss function (this is not correct when modelling binary data, but it makes the math easier). Evaluated on the whole training set, the MSE loss function is\n",
    "\\begin{equation}\n",
    "    J(\\mathbf{\\theta}) = \\frac{1}{4} \\sum_{\\mathbf{x} \\in \\mathbb{X}} (f^{*}(\\mathbf{x}) - f(\\mathbf{x};\\mathbf{\\theta}))^{2}. \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "We now choose the form of our model, $f(\\mathbf{x},\\mathbf{\\theta})$. If we choose a linear model with $\\mathbf{\\theta}$ consisting of $\\mathbf{w}$ and $b$ we have\n",
    "\\begin{equation}\n",
    "    f(\\mathbf{x};\\mathbf{w}, b) = \\mathbf{x}^{\\top} \\mathbf{w} + b \\tag{2}\n",
    "\\end{equation}\n",
    "We can minimize $J(\\mathbf{theat})$ in closed form with respect to $\\mathbf{\\theta}$ which yields $\\mathbf{w}=\\boldsymbol{0}$ and $b=\\frac{1}{2}$. This model output $0.5$ everywhere. This is because a linear model cannot have two different outputs for the same input; if $x_1 = 0$ then XOR can output either $1$ or $0$ depending on the value of $x_2$.\n",
    "\n",
    "We therefore want to implement a model that learns a different space in which a linear model is able to represent the solution. To do this we introduce a feedforward neural network with one hidden layer containing two hidden units $\\mathbf{h}$, which are computed by $f^{(1)}(\\mathbf{x};\\mathbf{W},\\mathbf{c})$. These outputs are used as input for the second layer, which is the output layer. This layer is still a linear function, but it is applied to $\\mathbf{h}$ rather than $\\mathbf{x}$. The complete model is then $\\mathbf{h} = f^{(1)}(\\mathbf{x};\\mathbf{W},\\mathbf{c})$ and $y = f^{(2)}(\\mathbf{h}; \\mathbf{w}, b)$ where $f(\\mathbf{x}; \\mathbf{W}, \\mathbf{c}, \\mathbf{w}, b) 0 f^{(2)}(f^{(1)}(\\mathbf{x}))$.\n",
    "\n",
    "To introduce non-linearity we do not have the activation function $f^{(1)}$ be linear. We choose another affine tranformation, $g$, and define $\\mathbf{h} = g(\\mathbf{W}^{\\top}\\mathbf{x} + \\mathbf{c})$, where $\\mathbf{W}$ provides the weights of a linear transformation and $\\mathbf{c}$ the biases. Previously to describe a linear transformation, we used a vector of weights and scalar bias parameters to describe an affine transformation. We now work with vectors $\\mathbf{x} \\mapsto \\mathbf{h}$, so we need a vector of bias parameters. We also choose the affine activation transformation $g$ to be a ReLU-function given by $g(z) = \\max\\{ 0,z \\}$. \n",
    "\n",
    "The entire network takes the form\n",
    "\\begin{equation}\n",
    "    f(\\mathbf{x};\\mathbf{W},\\mathbf{c},\\mathbf{w},b) = \\mathbf{w}^{\\top}\\max\\{ 0, \\mathbf{W}^{\\top}\\mathbf{x} + \\mathbf{c} \\} + b. \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Since this is a closed form we can minimize the loss. We find that this model yields a perfect solution. This is a short illustration of how we can use neural networks to fit model to non-linear spaces.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

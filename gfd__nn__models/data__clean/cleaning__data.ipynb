{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='font-family: CMU Sans Serif, sans-serif;'> Data cleaning  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have addressed the issue of missing data (in a barbaric way) we can move on to cleaning the data. This involves outlier detection and scaling. Like the \"Factor Investment\" book we will uniformized: for each point in time, the cross-sectional distribution of each feature is uniform over the unit interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from scipy.stats.mstats import winsorize\n",
    "from tqdm import tqdm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family: CMU Sans Serif, sans-serif;'> Data import  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the data where the missing values have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import primary features\n",
    "with open('listPrimaryFeaturesRemaining.json', 'r') as f:\n",
    "    listPrimaryFeatures = json.load(f)\n",
    "\n",
    "# Import dataframe cleaned for missing values\n",
    "dataGfdUs = pd.read_csv('usa__gfd__cleaned_v1.csv')\n",
    "\n",
    "# Define eom as a datetime object\n",
    "dataGfdUs['eom'] = pd.to_datetime(dataGfdUs['eom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## <span style='font-family: CMU Sans Serif, sans-serif;'> Outlier detection  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use winsorization to deal with outliers. This is done on a day-by-day and feature-by-feature basis. We will use `eom` as the date and the features to be winsorized are kept in `listPrimaryFeatures`. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_features(features, lower = 0.025, upper = 0.025):\n",
    "    return winsorize(features, limits = [lower, upper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates: 100%|██████████| 623/623 [02:38<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the already existing data\n",
    "dataGfdUs_v1 = dataGfdUs.copy()\n",
    "\n",
    "# Get all dates\n",
    "dates = dataGfdUs_v1['eom'].unique()\n",
    "\n",
    "# Winsorize data \n",
    "for date in tqdm(dates, desc=\"Processing dates\"):\n",
    "    for feature in listPrimaryFeatures:\n",
    "        dataGfdUs_v1.loc[dataGfdUs['eom'] == date, feature] = winsorize_features(dataGfdUs.loc[dataGfdUs['eom'] == date, feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family: CMU Sans Serif, sans-serif;'> Scaling data  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the outliers have been winsorized we move on to scaling. As as done in the book we will scale day-by-day and feature by feature. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dates: 100%|██████████| 623/623 [04:30<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize scaler\n",
    "scaler = QuantileTransformer(output_distribution = 'uniform')\n",
    "\n",
    "# Create a copy of the already existing data\n",
    "dataGfdUs_v2 = dataGfdUs_v1.copy()\n",
    "\n",
    "# Get all dates\n",
    "dates = dataGfdUs_v2['eom'].unique()\n",
    "\n",
    "# Scale data and ignore warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*n_quantiles.*greater than the total number of samples.*\")\n",
    "    \n",
    "    for date in tqdm(dates, desc=\"Processing dates\"):\n",
    "        for feature in listPrimaryFeatures:\n",
    "            dataGfdUs_v2.loc[dataGfdUs['eom'] == date, feature] = scaler.fit_transform(dataGfdUs_v1.loc[dataGfdUs_v1['eom'] == date, [feature]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='font-family: CMU Sans Serif, sans-serif;'> Saving data  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully cleaned data fully, we are ready to fit neural networks. We will this export the final cleaned data as a CSV, ready for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGfdUs_v2.to_csv(\"usa__gfd__cleaned_v2.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

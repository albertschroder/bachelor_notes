{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='font-family: CMU Sans Serif, sans-serif;'> Feed-Forward Neural-Networks  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family: CMU Sans Serif, sans-serif;'> Workflow  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we look at different feed-forward achitectures. To find the *optimal models* within each achitectures we implement grid search with successive handling to stop poorly performing models early. We use a pythonian approach to building the models (*i.e. we create modules for each model*). For architecture we will define the following hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyper-parameter               | Variable                                                           |\n",
    "|-------------------------------|--------------------------------------------------------------------|\n",
    "| Number of epochs              | $E$                                                                |\n",
    "| Amount of hidden layers       | $R$                                                                |\n",
    "| Amount of neuron per layer    | $n^{(r)}$                                                          |\n",
    "| Learning rate                 | $\\alpha$                                                           |\n",
    "| Mini-batch size               | $m$                                                                |\n",
    "| $L_1$ or $L_2$ regularization | $L_1$ or $L_2$                                                     |\n",
    "| Regularization factor         | $\\lambda$                                                          |\n",
    "| Optimization algorithm        | $\\text{Adam}$, or $\\text{RMSprop}$, or $\\text{SGD}$ with momentum. |\n",
    "| Momentum in $\\text{SGD}$      | $\\mu$                                                              |\n",
    "| Activation function           | $\\text{ReLU}$, $\\text{PReLU}$, $\\text{Leaky ReLU}$                 |\n",
    "| Dropout (bool)                | $\\text{Dropout}^{T}$ or $\\text{Dropout}^{F}$                       |\n",
    "| Dropout parameter             | $p$                                                                |\n",
    "| Last hidden layer $\\tanh$     | $\\text{True}$ or $\\text{False}$                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family: CMU Sans Serif, sans-serif;'> Package Import  </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "from typing import Union, List, Dict, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "import random\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "\n",
    "# Neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras.optimizers\n",
    "import keras.losses\n",
    "import keras.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family: CMU Sans Serif, sans-serif;'> Data </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family: CMU Sans Serif, sans-serif;'> Import  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we import the main dataframe and the list of remaining primary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main data\n",
    "data_gfd_usa = pd.read_csv('../data__clean/usa__gfd__cleaned_v2.csv')\n",
    "\n",
    "# Import primary features remaining\n",
    "with open('../data__clean/listPrimaryFeaturesRemaining.json', 'r') as f:\n",
    "    list_primary_features = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family: CMU Sans Serif, sans-serif;'> Train/Test Split  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split data into test and train for training and model validation. This is done below with the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels from data\n",
    "data_features = data_gfd_usa[list_primary_features]\n",
    "data_labels = data_gfd_usa['ret_exc_lead1m']\n",
    "\n",
    "# Split features and labels into test and train\n",
    "data_train_features, data_test_features, data_train_labels, data_test_labels = train_test_split(data_features, data_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='font-family: CMU Sans Serif, sans-serif;'> Building Modules  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is below: (1 custom classes that can be used on all models, and (2) specific classes for each architecture (or complexities within architectures).\n",
    "\n",
    "Below we define an optimizer class and a manager class we use to configure and manage the neural networks created. First we define some colors we would like to use when printing the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESET = \"\\033[0m\"\n",
    "CYAN  = \"\\033[1;36m\"\n",
    "MAG   = \"\\033[1;35m\"\n",
    "YEL   = \"\\033[1;33m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family: CMU Sans Serif, sans-serif;'> Get optimizer function  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things (*semi*) simple only 3 optimizers can be used: adam, SGD, RMSprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer: str = \"adam\", random_search = False, **kwags):\n",
    "    dflt_vals = {\n",
    "        \"sgd\": {\"learning_rate\": 0.01, \"momentum\": 0.0},\n",
    "        \"adam\": {\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"beta_1\": 0.9,\n",
    "            \"beta_2\": 0.999,\n",
    "            \"epsilon\": 1e-07,\n",
    "        },\n",
    "        \"rmsprop\": {\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"rho\": 0.9,\n",
    "            \"momentum\": 0.0,\n",
    "            \"epsilon\": 1e-07,\n",
    "            \"centered\": False,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Optimizer class for each optimizer\n",
    "    optimizer_classes = {\n",
    "        \"adam\": tf.keras.optimizers.Adam,\n",
    "        \"sgd\": tf.keras.optimizers.SGD,\n",
    "        \"rmsprop\": tf.keras.optimizers.RMSprop,\n",
    "    }\n",
    "\n",
    "    # Normalize optimizer name\n",
    "    optimizer_name = optimizer.lower()\n",
    "\n",
    "    # Check if optimizer is supported\n",
    "    if optimizer_name not in dflt_vals:\n",
    "        raise ValueError(f\"Optimizer '{optimizer_name}' not supported.\")\n",
    "\n",
    "    # Get default values for chosen optimizer\n",
    "    dflt = dflt_vals[optimizer_name]\n",
    "\n",
    "    # Get optimizer params (dict)\n",
    "    optimizer_params = {param: kwags.get(param, dflt[param]) for param in dflt}\n",
    "\n",
    "    # Get optimizer class\n",
    "    optimizer_class = optimizer_classes[optimizer_name]\n",
    "\n",
    "    if not random_search: \n",
    "        # Return optimizer with given or default params\n",
    "        print(\n",
    "            f\"{CYAN}INFO:{RESET} {f'[get_optimizer]':<25} {optimizer_name:<25} params: {optimizer_params}\"\n",
    "        )\n",
    "\n",
    "    return optimizer_class(**optimizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family: CMU Sans Serif, sans-serif;'> Model manager class  </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#   See this chat for additional ideas on model manager class https://chatgpt.com/c/67e5e6d4-c9b0-8004-98c4-375eb9e89c38\n",
    "\n",
    "\n",
    "class ModelManager:\n",
    "    # Class-level attribute to store the names of every model\n",
    "    _all_model_names = set()\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_model_names(cls):\n",
    "        return list(cls._all_model_names)\n",
    "\n",
    "    @classmethod\n",
    "    def remove_model_name(cls, model_name):\n",
    "        cls._all_model_names.discard(model_name)\n",
    "\n",
    "    def __init__(self, model, optimizer, model_name = None, random_search = False):\n",
    "\n",
    "        self.model = model\n",
    "        self.model_name = model_name if not random_search else None\n",
    "        self.random_search = random_search\n",
    "        self.optimizer = optimizer\n",
    "        self.callbacks = []\n",
    "        self.training_history = {}\n",
    "        if not random_search:\n",
    "            ModelManager._all_model_names.add(model_name)\n",
    "\n",
    "    def compile_model(self, loss: str = \"mse\", metrics: list = [\"mse\"]):\n",
    "        self.model.compile(optimizer=self.optimizer, loss=loss, metrics=metrics)\n",
    "        if not self.random_search:\n",
    "            params = inspect.getargvalues(inspect.currentframe()).locals\n",
    "            params.pop(\"self\", None)\n",
    "            print(\n",
    "                f\"{CYAN}INFO:{RESET} {f'[compile_model]':<25} {self.model_name:<25} params: {params}\"\n",
    "            )\n",
    "\n",
    "    def setup_callbacks(self, earlystop=False, checkpoint=False, **kwargs):\n",
    "        # Default parameter values for the callbacks\n",
    "        callback_map = {\n",
    "            \"earlystop\": (\n",
    "                EarlyStopping,\n",
    "                {\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    \"min_delta\": 1e-05,\n",
    "                    \"patience\": 10,\n",
    "                    \"verbose\": 0,\n",
    "                    \"mode\": \"auto\",\n",
    "                    \"baseline\": None,\n",
    "                    \"restore_best_weights\": False,\n",
    "                    \"start_from_epoch\": 0,\n",
    "                },\n",
    "            ),\n",
    "            \"checkpoint\": (\n",
    "                ModelCheckpoint,\n",
    "                {\n",
    "                    \"filepath\": \"test\",  # Placeholder filepath\n",
    "                    \"monitor\": \"val_loss\",\n",
    "                    \"verbose\": 0,\n",
    "                    \"save_best_only\": False,\n",
    "                    \"save_weights_only\": False,\n",
    "                    \"mode\": \"auto\",\n",
    "                    \"save_freq\": \"epoch\",\n",
    "                    \"initial_value_threshold\": None,\n",
    "                },\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Start with an empty list of callbacks\n",
    "        self.callbacks = []\n",
    "\n",
    "        # EarlyStopping Callback\n",
    "        if earlystop:\n",
    "            # Extract the class and default params from the callback_map\n",
    "            callback_class, default_params = callback_map[\"earlystop\"]\n",
    "            # Merge defaults with any kwargs passed by the user\n",
    "            merged_params = {\n",
    "                **default_params,\n",
    "                **{k: v for k, v in kwargs.items() if k in default_params},\n",
    "            }\n",
    "            earlystop_callback = callback_class(**merged_params)\n",
    "            self.callbacks.append(earlystop_callback)\n",
    "            if not self.random_search:\n",
    "                print(\n",
    "                    f\"{CYAN}INFO:{RESET} {f'[setup_callbacks]':<25} {self.model_name:<25} params: {merged_params}\"\n",
    "                )\n",
    "\n",
    "        # ModelCheckpoint Callback\n",
    "        if not self.random_search:\n",
    "            if checkpoint:\n",
    "                # Extract the class and default params from the callback_map\n",
    "                callback_class, default_params = callback_map[\"checkpoint\"]\n",
    "                # Create the 'check_points' folder if it doesn't exist\n",
    "                checkpoint_dir = \"check_points\"\n",
    "                if not os.path.exists(checkpoint_dir):\n",
    "                    os.makedirs(checkpoint_dir)\n",
    "\n",
    "                # Merge defaults with any kwargs passed by the user\n",
    "                merged_params = {\n",
    "                    **default_params,\n",
    "                    **{k: v for k, v in kwargs.items() if k in default_params},\n",
    "                }\n",
    "                # Use the model_name for the checkpoint file path\n",
    "                merged_params[\"filepath\"] = os.path.join(\n",
    "                    checkpoint_dir, f\"{self.model_name}_check_points.keras\"\n",
    "                )\n",
    "                checkpoint_callback = callback_class(**merged_params)\n",
    "                self.callbacks.append(checkpoint_callback)\n",
    "                print(\n",
    "                    f\"{CYAN}INFO:{RESET} {f'[setup_callbacks]':<25} {self.model_name:<25} params: {merged_params}\"\n",
    "                )\n",
    "\n",
    "    def train_model(\n",
    "        self,\n",
    "        x,\n",
    "        y,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        validation_split=0.2,\n",
    "    ):\n",
    "        if not self.random_search:\n",
    "            params = inspect.getargvalues(inspect.currentframe()).locals\n",
    "            params.pop(\"x\", None)\n",
    "            params.pop(\"y\", None)\n",
    "            params.pop(\"self\", None)\n",
    "            print(\n",
    "                f\"{CYAN}INFO:{RESET} {f'[train_model]':<25} {self.model_name:<25} params: {params}\" \n",
    "            )\n",
    "\n",
    "        history = self.model.fit(\n",
    "            x,\n",
    "            y,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose,\n",
    "            callbacks=self.callbacks,\n",
    "            validation_split=validation_split,\n",
    "        )\n",
    "\n",
    "        self.training_history = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family: CMU Sans Serif, sans-serif;'> Random search optimizer  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a random search module for finding the *best* hyperparameters within each model/architecture. When defining seach space we define the type of each parameter: 'uncond' or 'cond'. This will determine if the search parameter is conditional on other parameters. If the search parameter is conditional, we check if the condition is met. \n",
    "\n",
    "First we need to define the distributions from which the numerical hyperparameteres are drawn. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log normal distribution \n",
    "def log_norm(min, max):\n",
    "    log_min = np.log(min)\n",
    "    log_max = np.log(max)\n",
    "    mu = (log_min + log_max) / 2\n",
    "    sigma = (log_max - log_min)/ 2\n",
    "    return np.random.lognormal(mu, sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function which get random parameters specified by the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random parameter from search_space\n",
    "def get_rand_param(param_info: dict, params: dict):\n",
    "    # Check if conditional\n",
    "    if param_info['type'] == 'cond':\n",
    "        \n",
    "        # Check if condition met.\n",
    "        if param_info['condition'](params):\n",
    "\n",
    "            # Check if distribution should be used for rand generation\n",
    "            if type(param_info['param_space'][0]) == float:\n",
    "                return log_norm(param_info['param_space'][0], param_info['param_space'][1])\n",
    "            # If not select rand element from array\n",
    "            else:\n",
    "                return random.choice(param_info['param_space'])\n",
    "        # If condition not met retun None\n",
    "        else:\n",
    "            return None\n",
    "    # If not a conditional get rand\n",
    "    else:\n",
    "        # If float find rand in distrbution \n",
    "        if type(param_info['param_space'][0]) == float:\n",
    "                return log_norm(param_info['param_space'][0], param_info['param_space'][1])\n",
    "        # Else find rand element in array\n",
    "        else:\n",
    "            return random.choice(param_info['param_space'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# Make it so i can parse constant values to functions (e.g. by creating a new dictionary)\n",
    "# Define random search function\n",
    "def random_search(n_trails: int, search_space: dict, model_constructor, x_train, y_train):\n",
    "    best_val_loss = float('inf')  \n",
    "    best_result = None\n",
    " \n",
    "    for trail in range(n_trails):\n",
    "        print(f\"Trail: {trail + 1}\")\n",
    "        # Define dictionaries with the randomly generated params (sep separates by 'placement')\n",
    "        params = {}\n",
    "        sep_params = {}\n",
    "\n",
    "        for param in search_space.keys():\n",
    "            rand_param = get_rand_param(search_space[param], params)\n",
    "\n",
    "            if rand_param is not None:\n",
    "                params[param] = rand_param\n",
    "                placement = search_space[param]['placement']\n",
    "\n",
    "                if placement not in sep_params:\n",
    "                    sep_params[placement] = {}\n",
    "\n",
    "                sep_params[placement][param] = rand_param\n",
    "\n",
    "        optimizer = get_optimizer(**sep_params.get('get_optimizer', {}), random_search=True) \n",
    "        model = model_constructor(**sep_params.get('model_constructor', {}), random_search=True)\n",
    "        model_mgmt = ModelManager(model = model, optimizer=optimizer, random_search=True)\n",
    "        model_mgmt.compile_model(metrics=['mse', 'mae'])\n",
    "        model_mgmt.setup_callbacks(True, restore_best_weights = True)\n",
    "        model_mgmt.train_model(x=x_train, y=y_train, epochs=100, verbose=0, **sep_params.get('train_model', {}))\n",
    "        val_loss = model_mgmt.training_history.history['val_loss'][-1]\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_result = {**params, 'val_loss': val_loss}\n",
    "\n",
    "    return best_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='font-family: CMU Sans Serif, sans-serif;'> Building networks  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family: CMU Sans Serif, sans-serif;'> Vanilla FFN  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family: CMU Sans Serif, sans-serif;'> Vanilla FNN function  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a function for building vanilla feed forward neural networks.\n",
    "\n",
    "We allow for varying the following parameters dynamically:\n",
    "- number of neurons in each layer;\n",
    "- type of activation function used in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "# Make it so if only one argument is passed for layer_neurons and activations, then that becomes the standard for all the layers.\n",
    "def build_vanilla_fnn(\n",
    "    input_shape=(141,), layer_neurons=[32, 16, 1], activations=[\"relu\", \"tanh\", None], random_search = False\n",
    "):\n",
    "    # Same length of inputs\n",
    "    assert len(layer_neurons) == len(activations), \"layer_neurons and activations must have same length.\"\n",
    "    model_id = str(uuid.uuid4())\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "\n",
    "    for i, (units, activation) in enumerate(zip(layer_neurons[:-1], activations[:-1])):\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(\n",
    "                units, activation=activation, name=f\"{model_id}_dense_{i+1}\"\n",
    "            )\n",
    "        )\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            layer_neurons[-1],\n",
    "            activation=activations[-1],\n",
    "            name=f\"{model_id}_dense_{len(layer_neurons)}\",\n",
    "        )\n",
    "    )\n",
    "    if not random_search:\n",
    "        params = inspect.getargvalues(inspect.currentframe()).locals\n",
    "        params.pop(\"model_id\", None)\n",
    "        params.pop(\"model\", None)\n",
    "        params.pop(\"i\", None)\n",
    "        params.pop(\"units\", None)\n",
    "        params.pop(\"activation\", None)\n",
    "\n",
    "        print(\n",
    "            f\"{CYAN}INFO:{RESET} {f'[build_vanilla_fnn]':<51} params: {params}\"\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family: CMU Sans Serif, sans-serif;'> Creating a NN  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lest try our creating a first neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test search space \n",
    "search_space = {\n",
    "    'optimizer': {'type': 'uncond', 'param_space': ['adam', 'sgd', 'rmsprop'], 'placement': 'get_optimizer'},\n",
    "    'batch_size': {'type': 'uncond', 'param_space': [32, 64, 128], 'placement': 'train_model'},\n",
    "    'momentum': {'type': 'cond', 'condition': lambda params: params['optimizer'] in ['sgd', 'rmsprop'], 'param_space': [0.2, 0.7], 'placement': 'get_optimizer'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trail: 1\n",
      "Trail: 2\n",
      "Trail: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'optimizer': 'adam', 'batch_size': 32, 'val_loss': 0.016122953966259956}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search(3, search_space, build_vanilla_fnn, data_train_features, data_train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelor_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
